{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bbf5f6",
   "metadata": {},
   "source": [
    "# Sentence Simplification\n",
    "\n",
    "https://github.com/szymeklimek/BEng-Thesis---Semantic-Analysis/blob/89552577f77a40e8cce89296fe45aac86adda6b9/src/simple_sentences_algorithm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80ccde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycorenlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "44aed122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSentence:\n",
    "    \n",
    "    def __init__(self,uri=\"http://localhost:9000/\" ):\n",
    "        self.nlp = StanfordCoreNLP(uri)\n",
    "        self.prop = {\n",
    "                \"timeout\": \"50000\",\n",
    "                \"annotators\":\"tokenize, pos, lemma, openie, depparse\",\n",
    "                \"outputFormat\": \"json\",\n",
    "                \"openie.max_entailments_per_clause\":\"1\", # no of triples from each clause\n",
    "                \"openie.triple.strict\":\"true\",\n",
    "            #'openie.affinity_probability_cap': 2 / 3\n",
    "        }\n",
    "   \n",
    "    def _check_if_found(self, found, idx_gov, idx_dep):\n",
    "        if idx_gov in found or idx_dep in found:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _get_subj_indexes(self, sentence):\n",
    "        sub_indexes = []\n",
    "        for idx, dep in enumerate(sentence['basicDependencies']):\n",
    "            if 'nsubj' in str(dep['dep']):\n",
    "                sub_indexes.append(idx)\n",
    "        return sub_indexes\n",
    "\n",
    "    def _check_if_simple_sentence(self, sub_count):\n",
    "        if len(sub_count) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _reccurent_find_all(self, deps_to_consider, found=[], to_find=[]):\n",
    "        deps = []\n",
    "        temp_deps_to_consider = deps_to_consider[:]\n",
    "        for idx, dep in enumerate(temp_deps_to_consider):\n",
    "            if dep['governor'] in to_find or dep['dependent'] in to_find:\n",
    "                if not self._check_if_found(found, dep['governor'], dep['dependent']):\n",
    "                    deps.append(dep)\n",
    "                    new_deps_to_consider = temp_deps_to_consider[:]\n",
    "                    new_deps_to_consider.pop(idx)\n",
    "                    to_find_idx = 0\n",
    "                    if dep['governor'] in to_find:\n",
    "                        to_find_idx = dep['dependent']\n",
    "                    else:\n",
    "                        to_find_idx = dep['governor']\n",
    "                    more_deps = self._reccurent_find_all(new_deps_to_consider, to_find, [to_find_idx])\n",
    "                    if more_deps:\n",
    "                        deps = deps + more_deps\n",
    "        return deps\n",
    "\n",
    "    def _find_connected_deps(self, sub, deps_to_consider):\n",
    "        gov_id = sub['governor']\n",
    "        dep_id = sub['dependent']\n",
    "        deps = []\n",
    "        idx_sub = 0\n",
    "        temp_deps_to_consider = deps_to_consider[:]\n",
    "        for idx, dep in enumerate(deps_to_consider):\n",
    "            if dep['governor'] == sub['governor'] and dep['dependent'] == sub['dependent']:\n",
    "                idx_sub = idx\n",
    "        temp_deps_to_consider.pop(idx_sub)\n",
    "        deps = self._reccurent_find_all(temp_deps_to_consider, to_find=[gov_id, dep_id])\n",
    "        deps.append(sub)\n",
    "        return deps\n",
    "\n",
    "    def _get_dependencies_tree_from_compound(self, sentence, sub_indexes):\n",
    "        deps_to_consider = []\n",
    "        for dep in sentence['basicDependencies']:\n",
    "            if str(dep['dep']) not in ['mark', 'acl', 'appos', 'advcl', 'cc', 'ccomp', 'conj', 'dep', 'parataxis',\n",
    "                                       'ref', 'punct', 'acl:relcl', 'det']:\n",
    "                deps_to_consider.append(dep)\n",
    "        # build deps trees for nsubjs\n",
    "        dependencies = []\n",
    "        for sub_idx in sub_indexes:\n",
    "            sub = sentence['basicDependencies'][sub_idx]\n",
    "            dependencies.append(self._find_connected_deps(sub, deps_to_consider))\n",
    "        return dependencies\n",
    "\n",
    "    def _glue_words_into_sentences(self, dependencies):\n",
    "        index_to_word = {}\n",
    "        for deps in dependencies:\n",
    "            if deps['governor'] > 0:\n",
    "                index_to_word[deps['governor']] = deps['governorGloss']\n",
    "            if deps['dependent'] > 0:\n",
    "                index_to_word[deps['dependent']] = deps['dependentGloss']\n",
    "\n",
    "        ordered_words = [index_to_word[k] for k in sorted(index_to_word)]\n",
    "        return \" \".join(ordered_words)\n",
    "\n",
    "\n",
    "    def get_simple_sentences(self, text):\n",
    "        res = self.nlp.annotate(text, properties=self.prop)\n",
    "        out = json.loads(res)\n",
    "        sentence = out['sentences'][0]\n",
    "        # start\n",
    "        simple_sentences = []\n",
    "        sub_indexes = self._get_subj_indexes(sentence)\n",
    "        simple_sentence_deps = self._get_dependencies_tree_from_compound(sentence, sub_indexes)\n",
    "        for deps in simple_sentence_deps:\n",
    "            sentence = self._glue_words_into_sentences(deps)\n",
    "            simple_sentences.append(sentence)\n",
    "\n",
    "        return simple_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dcfef40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplet(text, uri=\"http://localhost:9000/\"):\n",
    "    uri = nlp=StanfordCoreNLP(uri)\n",
    "    properties = {\n",
    "            \"timeout\": \"50000\",\n",
    "            \"annotators\":\"tokenize, pos, lemma, openie, depparse\",\n",
    "            \"outputFormat\": \"json\",\n",
    "            \"openie.max_entailments_per_clause\":\"3\", # no of triples from each clause\n",
    "            \"openie.triple.strict\":\"true\",\n",
    "          'openie.affinity_probability_cap': 2 / 3\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    res = nlp.annotate(text, properties=properties)\n",
    "    out = json.loads(res)\n",
    "    triplet = [(i['subject'],i['relation'],i['object']) for i in out['sentences'][0]['openie']]\n",
    "    return triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "685c6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SimpleSentenceGenerationAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "41bdfcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank should not share non-public OCC information']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detect if sentence is having -ve polarity, remove negation, extract triple, then add negation\n",
    "\n",
    "text = \"The Bank should not share any non-public OCC information\"\n",
    "text_c = sc.get_simple_sentences(text)\n",
    "text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "03eae724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bank', 'should share', 'non-public OCC information')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_triplet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a556363a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bank', 'should share', 'OCC information'),\n",
       " ('Bank', 'should share', 'non-public OCC information')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_triplet(text_c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "679719d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = \"bank management should conduct due diligence of foreign-based service provider before contacting with provider and distributor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8d05c724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bank management should conduct due diligence of foreign based service provider before contacting with provider']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_c = sc.get_simple_sentences(t2)\n",
    "text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "21a02fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bank management', 'should conduct due', 'diligence'),\n",
       " ('bank management',\n",
       "  'should conduct due',\n",
       "  'diligence of foreign based service provider'),\n",
       " ('bank management', 'should conduct due', 'diligence of service provider')]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_triplet(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b7fbf77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love pen']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"I love pen but not paper\"\n",
    "text_c = sc.get_simple_sentences(txt)\n",
    "text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb177e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dcef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
